{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "from tensorflow import random\n",
    "random.set_seed(404)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_FILE_PATH = \"C:/Users/BediZ/DS-ML/Jupyter Notebook - ML Projects/resources/MNIST/digit_xtrain.csv\"\n",
    "Y_TRAIN_FILE_PATH = \"C:/Users/BediZ/DS-ML/Jupyter Notebook - ML Projects/resources/MNIST/digit_ytrain.csv\"\n",
    "X_TEST_FILE_PATH = \"C:/Users/BediZ/DS-ML/Jupyter Notebook - ML Projects/resources/MNIST/digit_xtest.csv\"\n",
    "Y_TEST_FILE_PATH = \"C:/Users/BediZ/DS-ML/Jupyter Notebook - ML Projects/resources/MNIST/digit_ytest.csv\"\n",
    "IMAGE_TEST_PATH = \"C:/Users/BediZ/DS-ML/Jupyter Notebook - ML Projects/resources/MNIST/test_img.png\"\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_logs/'\n",
    "\n",
    "NUMBER_OF_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "NUMBER_OF_CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH * IMAGE_HEIGHT * NUMBER_OF_CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_FILE_PATH, delimiter=',', dtype=int)\n",
    "y_test = np.loadtxt(Y_TEST_FILE_PATH, delimiter=',', dtype=int)\n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_FILE_PATH, delimiter=',', dtype=int)\n",
    "x_test = np.loadtxt(X_TEST_FILE_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale\n",
    "x_train_all, x_test = x_train_all / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converte target to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values= y_train_all[:5]\n",
    "np.eye(NUMBER_OF_CLASSES)[values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all = np.eye(NUMBER_OF_CLASSES)[y_train_all]\n",
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NUMBER_OF_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation dataset from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_validation = x_train_all[:VALIDATION_SIZE]\n",
    "y_validation = y_train_all[:VALIDATION_SIZE]\n",
    "\n",
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NUMBER_OF_CLASSES], name='labelsY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "#### Hyperparameters - detirmen by me - how long to train model - number of epochs - learning rate - number of layers - number of nodes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 50\n",
    "learning_rate = 1e-4 #0.0001 - 1st test\n",
    "learning_rate = 1e-3 #0.001 - 2nd test run\n",
    "number_of_neurons_hidden1 = 512\n",
    "number_of_neurons_hidden2 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for 1st part of module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('hidden_output_layer_1'):\n",
    "#     # 1st layer of neurons\n",
    "#     initial_weights1= tf.truncated_normal(shape = [TOTAL_INPUTS, number_of_neurons_hidden1], stddev = 0.1, seed = 42)\n",
    "#     weights1 = tf.Variable(initial_value = initial_weights1, name='Weight_1')\n",
    "#     # where the network is learning\n",
    "#     initial_bias1 = tf.constant(value=0.0, shape=[number_of_neurons_hidden1])\n",
    "#     bias1 = tf.Variable(initial_value = initial_bias1, name='Bias_1')\n",
    "#     in_layer1 = tf.matmul(X, weights1) + bias1\n",
    "#     out_layer1 = tf.nn.relu(in_layer1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('hidden_output_layer_2'):\n",
    "#     # 2nd layer of neural\n",
    "#     initial_weights2= tf.truncated_normal(shape = [number_of_neurons_hidden1, number_of_neurons_hidden2], \n",
    "#                                           stddev = 0.1, \n",
    "#                                           seed = 42)\n",
    "#     weights2 = tf.Variable(initial_value = initial_weights2, name='Weight_2')\n",
    "#     # where the network is learning\n",
    "#     initial_bias2 = tf.constant(value=0.0, shape=[number_of_neurons_hidden2])\n",
    "#     bias2 = tf.Variable(initial_value = initial_bias2, name='Bieas_2')\n",
    "\n",
    "#     in_layer2 = tf.matmul(out_layer1, weights2) + bias2\n",
    "#     out_layer2 = tf.nn.relu(in_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('output_layer'):\n",
    "\n",
    "#     # out - last layer of neural -> finis with the size od classes - softmax for output\n",
    "#     initial_weights3= tf.truncated_normal(shape = [number_of_neurons_hidden2, NUMBER_OF_CLASSES], \n",
    "#                                           stddev = 0.1, \n",
    "#                                           seed = 42)\n",
    "#     weights3 = tf.Variable(initial_value = initial_weights3, name='Weight_3')\n",
    "#     # where the network is learning\n",
    "#     initial_bias3 = tf.constant(value=0.0, shape=[NUMBER_OF_CLASSES])\n",
    "#     bias3 = tf.Variable(initial_value = initial_bias3, name='Bias_3')\n",
    "\n",
    "#     in_layer3 = tf.matmul(out_layer2, weights3) + bias3\n",
    "#     output_out_layer3 = tf.nn.softmax(in_layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(initial_previous_out_layer, weight_dimensions, bias_dimensions, name):\n",
    "    with tf.name_scope(name):\n",
    "\n",
    "        # out - last layer of neural -> finis with the size od classes - softmax for output\n",
    "        initial_weights= tf.truncated_normal(\n",
    "            shape = weight_dimensions,\n",
    "            stddev = 0.1, \n",
    "            seed = 42)\n",
    "        weights = tf.Variable(initial_value = initial_weights, name='Weight')\n",
    "        # where the network is learning\n",
    "        initial_bias = tf.constant(value=0.0, shape=bias_dimensions)\n",
    "        bias = tf.Variable(initial_value = initial_bias, name='Bias')\n",
    "\n",
    "        in_layer = tf.matmul(initial_previous_out_layer, weights) + bias\n",
    "        \n",
    "        if name == 'out':        \n",
    "            output_out_layer = tf.nn.softmax(in_layer)\n",
    "        else:            \n",
    "            output_out_layer = tf.nn.relu(in_layer)\n",
    "            \n",
    "            \n",
    "        tf.summary.histogram('weights', weights)\n",
    "        tf.summary.histogram('bias', bias)    \n",
    "        \n",
    "        return output_out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without dropout\n",
    "# out_layer1 = setup_layer(\n",
    "#     X, \n",
    "#     weight_dimensions=[TOTAL_INPUTS, number_of_neurons_hidden1], \n",
    "#     bias_dimensions=[number_of_neurons_hidden1], \n",
    "#     name='out_layer1')\n",
    "# out_layer2 = setup_layer(\n",
    "#     out_layer1, \n",
    "#     weight_dimensions=[number_of_neurons_hidden1, number_of_neurons_hidden2], \n",
    "#     bias_dimensions=[number_of_neurons_hidden2], \n",
    "#     name='out_layer2')\n",
    "# output_out_layer3 = setup_layer(\n",
    "#     out_layer2, \n",
    "#     weight_dimensions=[number_of_neurons_hidden2, NUMBER_OF_CLASSES], \n",
    "#     bias_dimensions=[NUMBER_OF_CLASSES], \n",
    "#     name='output_out_layer3')\n",
    "\n",
    "# model_name= f'{number_of_neurons_hidden1}-{number_of_neurons_hidden2} LR{learning_rate} E{number_of_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-7dedec75d0f1>:11: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "out_layer1 = setup_layer(\n",
    "    X, \n",
    "    weight_dimensions=[TOTAL_INPUTS, number_of_neurons_hidden1], \n",
    "    bias_dimensions=[number_of_neurons_hidden1], \n",
    "    name='out_layer1')\n",
    "\n",
    "#dropout layer\n",
    "out_layer_drop = tf.nn.dropout(\n",
    "    out_layer1, \n",
    "    keep_prob=0.8, \n",
    "    name='out_layer_drop')\n",
    "\n",
    "out_layer2 = setup_layer(\n",
    "    out_layer_drop, \n",
    "    weight_dimensions=[number_of_neurons_hidden1, number_of_neurons_hidden2], \n",
    "    bias_dimensions=[number_of_neurons_hidden2], \n",
    "    name='out_layer2')\n",
    "output_out_layer3 = setup_layer(\n",
    "    out_layer2, \n",
    "    weight_dimensions=[number_of_neurons_hidden2, NUMBER_OF_CLASSES], \n",
    "    bias_dimensions=[NUMBER_OF_CLASSES], \n",
    "    name='output_out_layer3')\n",
    "\n",
    "model_name= f'{number_of_neurons_hidden1}-DO-{number_of_neurons_hidden2} LR{learning_rate} E{number_of_epochs}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder successfuly created at location: tensorboard_mnist_digit_logs/512-DO-64 LR0.001 E50_at_10_15_49\n"
     ]
    }
   ],
   "source": [
    "# Folder for Tensorboard\n",
    "\n",
    "folder_name = f'{model_name}_at_{strftime(\"%H_%M_%S\")}'\n",
    "directory_tensorborad = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory_tensorborad)\n",
    "except OSError as exception:\n",
    "        print('Folder NOT created at location:', directory_tensorborad)\n",
    "        print(exception.strerror)\n",
    "else:\n",
    "    print('Folder successfuly created at location:', directory_tensorborad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimization and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_calculation'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output_out_layer3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calculation'):\n",
    "    model_prediction = tf.argmax(output_out_layer3, axis=1, name='prediction')\n",
    "    correct_prediction = tf.equal(model_prediction, tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss-cost', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "file_writer = tf.summary.FileWriter(directory_tensorborad + '/train')\n",
    "file_writer.add_graph(session.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_writer = tf.summary.FileWriter(directory_tensorborad + '/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialized_variables = tf.global_variables_initializer()\n",
    "session.run(initialized_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights1.eval(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bias3.eval(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = y_train.shape[0]\n",
    "number_of_iteratons = int(number_of_examples / size_of_batch)\n",
    "\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data_set, labels):\n",
    "    global number_of_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start_point = index_in_epoch\n",
    "    index_in_epoch = index_in_epoch + batch_size\n",
    "    \n",
    "    if index_in_epoch > number_of_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    \n",
    "    end_point = index_in_epoch\n",
    "    \n",
    "    return data_set[start_point:end_point], labels[start_point:end_point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0 \t- Training Accuracy = 0.8360000252723694\n",
      "Epoch number 1 \t- Training Accuracy = 0.8560000061988831\n",
      "Epoch number 2 \t- Training Accuracy = 0.9639999866485596\n",
      "Epoch number 3 \t- Training Accuracy = 0.9810000061988831\n",
      "Epoch number 4 \t- Training Accuracy = 0.9829999804496765\n",
      "Epoch number 5 \t- Training Accuracy = 0.9860000014305115\n",
      "Epoch number 6 \t- Training Accuracy = 0.984000027179718\n",
      "Epoch number 7 \t- Training Accuracy = 0.9869999885559082\n",
      "Epoch number 8 \t- Training Accuracy = 0.9900000095367432\n",
      "Epoch number 9 \t- Training Accuracy = 0.9900000095367432\n",
      "Epoch number 10 \t- Training Accuracy = 0.9879999756813049\n",
      "Epoch number 11 \t- Training Accuracy = 0.9890000224113464\n",
      "Epoch number 12 \t- Training Accuracy = 0.9919999837875366\n",
      "Epoch number 13 \t- Training Accuracy = 0.9919999837875366\n",
      "Epoch number 14 \t- Training Accuracy = 0.9900000095367432\n",
      "Epoch number 15 \t- Training Accuracy = 0.9940000176429749\n",
      "Epoch number 16 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 17 \t- Training Accuracy = 0.9940000176429749\n",
      "Epoch number 18 \t- Training Accuracy = 0.9950000047683716\n",
      "Epoch number 19 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 20 \t- Training Accuracy = 0.9980000257492065\n",
      "Epoch number 21 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 22 \t- Training Accuracy = 0.9940000176429749\n",
      "Epoch number 23 \t- Training Accuracy = 0.9980000257492065\n",
      "Epoch number 24 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 25 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 26 \t- Training Accuracy = 0.9959999918937683\n",
      "Epoch number 27 \t- Training Accuracy = 0.9959999918937683\n",
      "Epoch number 28 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 29 \t- Training Accuracy = 0.9980000257492065\n",
      "Epoch number 30 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 31 \t- Training Accuracy = 0.9980000257492065\n",
      "Epoch number 32 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 33 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 34 \t- Training Accuracy = 0.9980000257492065\n",
      "Epoch number 35 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 36 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 37 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 38 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 39 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 40 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 41 \t- Training Accuracy = 0.9980000257492065\n",
      "Epoch number 42 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 43 \t- Training Accuracy = 0.9980000257492065\n",
      "Epoch number 44 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 45 \t- Training Accuracy = 0.9980000257492065\n",
      "Epoch number 46 \t- Training Accuracy = 0.9990000128746033\n",
      "Epoch number 47 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 48 \t- Training Accuracy = 0.996999979019165\n",
      "Epoch number 49 \t- Training Accuracy = 0.9980000257492065\n",
      "Done training!\n",
      "In anaconda prompt type: \n",
      " tensorboard --logdir=\"C:\\Users\\BediZ\\DS-ML\\Jupyter Notebook - ML Projects\\05 Neural Nets - Retrainer Image Classification\\tensorboard_mnist_digit_logs\" \n",
      " and you will get a link to TensorBoard linke: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(number_of_epochs):\n",
    "    \n",
    "    # ========== Training Dataset ========== #\n",
    "    for i in range(number_of_iteratons):\n",
    "        batch_x, batch_y = next_batch(batch_size=size_of_batch, data_set = x_train, labels=y_train)\n",
    "        #print('x', batch_x.shape, 'y', batch_y.shape)\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        session.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    s, batch_accuracy = session.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "    \n",
    "    file_writer.add_summary(s, epoch)\n",
    "    \n",
    "    print(f'Epoch number {epoch} \\t- Training Accuracy = {batch_accuracy}')\n",
    "    # ========== Validation ========== #\n",
    "    summery = session.run(fetches=merged_summary, feed_dict={X:x_validation, Y:y_validation})\n",
    "    validation_writer.add_summary(summery, epoch)\n",
    "print('Done training!') \n",
    "print('In anaconda prompt type: \\n tensorboard --logdir=\\\"C:\\\\Users\\\\BediZ\\\\DS-ML\\\\Jupyter Notebook - ML Projects\\\\05 Neural Nets - Retrainer Image Classification\\\\tensorboard_mnist_digit_logs\\\" \\n and you will get a link to TensorBoard linke: http://localhost:6006/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.compat.v1.compat' has no attribute 'v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-191fd08017dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'accuracy_calculation/prediction'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmodel_prediction\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SavedModel'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.compat.v1.compat' has no attribute 'v1'"
     ]
    }
   ],
   "source": [
    "outputs = {'accuracy_calculation/prediction':model_prediction}\n",
    "inputs = {'X':X}\n",
    "tf.saved_model.simple_save(session, 'SavedModel', inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test = Image.open(\"C:/Users/BediZ/DS-ML/Jupyter Notebook - ML Projects/resources/MNIST/test_img.png\")\n",
    "image_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test_black_white = image_test.convert(mode='L')\n",
    "image_array = np.invert(image_test_black_white)\n",
    "image_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_flat = image_array.ravel()\n",
    "image_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test_prediction = session.run(fetches=tf.argmax(output_out_layer3, axis=1), feed_dict={X:[image_flat]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prediction for test image _2_ is: {image_test_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = session.run(fetches=accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "print(f'Prediction accuracy for test test set is: {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()\n",
    "validation_writer.close()\n",
    "session.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
